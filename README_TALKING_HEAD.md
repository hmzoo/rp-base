# API Talking Head - RunPod Serverless

G√©n√®re des vid√©os o√π une personne sur une image "lit" un texte fourni (avatar parlant / digital human).

## üéØ Fonctionnalit√©

**Input:**
- Une image (photo d'une personne)
- Un texte √† faire lire

**Output:**
- Une vid√©o o√π la personne "lit" le texte avec mouvements de bouche synchronis√©s

## üìã Architecture

```
Image + Texte
    ‚Üì
1. Text-to-Speech (TTS) ‚Üí Audio
    ‚Üì
2. Wav2Lip/SadTalker ‚Üí Vid√©o avec lip-sync
    ‚Üì
3. Upload S3 ‚Üí URL publique
```

## üöÄ Installation rapide

### Option 1: Version simple avec gTTS

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Installer les d√©pendances de base
pip install gTTS requests Pillow

# Tester
python test_talking_head.py
```

### Option 2: Version compl√®te avec Wav2Lip (GPU requis)

```bash
# 1. Cloner Wav2Lip
git clone https://github.com/Rudrabha/Wav2Lip.git
cd Wav2Lip

# 2. T√©l√©charger le mod√®le pr√©-entra√Æn√©
wget 'https://github.com/Rudrabha/Wav2Lip/releases/download/models/wav2lip_gan.pth' -O 'models/wav2lip_gan.pth'

# 3. Installer les d√©pendances
pip install -r requirements.txt

# 4. Retourner au projet
cd ..
```

## üé¨ Utilisation

### Format de l'API

```json
{
  "input": {
    "image": "https://example.com/photo.jpg",  // ou base64
    "text": "Bonjour, je suis un avatar virtuel.",
    "language": "fr",  // optionnel: fr, en, es, etc.
    "voice": "default"  // optionnel
  }
}
```

### Exemple Python

```python
import runpod

runpod.api_key = "votre-cl√©-api"
endpoint = runpod.Endpoint("votre-endpoint-id")

result = endpoint.run_sync({
    "input": {
        "image": "https://example.com/photo.jpg",
        "text": "Bonjour, je suis un avatar cr√©√© avec RunPod.",
        "language": "fr"
    }
})

print(f"Vid√©o: {result['video_url']}")
```

### Exemple avec image base64

```python
import base64

with open('photo.jpg', 'rb') as f:
    image_b64 = base64.b64encode(f.read()).decode('utf-8')

result = endpoint.run_sync({
    "input": {
        "image": f"data:image/jpeg;base64,{image_b64}",
        "text": "Test avec image locale."
    }
})
```

## üîß Configuration des services

### 1. Text-to-Speech (TTS)

Choisissez un service TTS selon vos besoins:

#### Option A: gTTS (Gratuit, basique)
```bash
pip install gTTS
```
‚úÖ Gratuit  
‚ùå Qualit√© basique  
‚ùå Voix limit√©es

#### Option B: ElevenLabs (Recommand√©, payant)
```bash
pip install elevenlabs
```
```python
# Dans handler_talking_head.py, modifiez text_to_speech():
from elevenlabs import generate, set_api_key

set_api_key(os.environ.get('ELEVENLABS_API_KEY'))
audio = generate(text=text, voice="Bella")
```
‚úÖ Excellente qualit√©  
‚úÖ Voix naturelles  
üí∞ Payant (~$0.30/1K caract√®res)

#### Option C: Azure TTS (Professionnel)
```bash
pip install azure-cognitiveservices-speech
```
‚úÖ Qualit√© professionnelle  
‚úÖ Multi-langues  
üí∞ Payant (~$1/1M caract√®res)

#### Option D: Coqui TTS (Open source)
```bash
pip install TTS
```
‚úÖ Open source  
‚úÖ Bonne qualit√©  
‚ö†Ô∏è N√©cessite GPU

### 2. Mod√®le Talking Head

#### Option A: Wav2Lip (Recommand√©)

**Avantages:**
- Lip-sync pr√©cis
- Rapide
- Mod√®le mature

**Installation:**
```bash
# Cloner le repo
git clone https://github.com/Rudrabha/Wav2Lip.git
cd Wav2Lip

# T√©l√©charger le mod√®le
wget 'https://github.com/Rudrabha/Wav2Lip/releases/download/models/wav2lip_gan.pth' \
  -O 'checkpoints/wav2lip_gan.pth'

# Installer
pip install -r requirements.txt
```

**Int√©gration dans handler_talking_head.py:**
```python
import sys
sys.path.append('./Wav2Lip')
from inference import main as wav2lip_inference

def generate_talking_head(image_path, audio_path, output_path):
    wav2lip_inference(
        checkpoint_path='Wav2Lip/checkpoints/wav2lip_gan.pth',
        face=image_path,
        audio=audio_path,
        outfile=output_path,
        static=False,
        fps=25,
        resize_factor=1
    )
    return output_path
```

#### Option B: SadTalker (Meilleure qualit√©)

**Avantages:**
- Mouvements de t√™te naturels
- Expressions faciales
- Meilleure qualit√© visuelle

**Installation:**
```bash
git clone https://github.com/OpenTalker/SadTalker.git
cd SadTalker
pip install -r requirements.txt

# T√©l√©charger les mod√®les (automatique au premier run)
```

**Plus exigeant en ressources (GPU puissant recommand√©)**

#### Option C: D-ID API (Cloud, le plus simple)

```bash
pip install requests
```

```python
import requests

def generate_talking_head(image_url, audio_url, output_path):
    response = requests.post(
        'https://api.d-id.com/talks',
        headers={
            'Authorization': f'Basic {D_ID_API_KEY}',
            'Content-Type': 'application/json'
        },
        json={
            'source_url': image_url,
            'script': {
                'type': 'audio',
                'audio_url': audio_url
            }
        }
    )
    # R√©cup√©rer la vid√©o une fois pr√™te
    video_url = response.json()['result_url']
    return video_url
```

‚úÖ Tr√®s simple  
‚úÖ Qualit√© professionnelle  
üí∞ Payant (~$0.10/vid√©o)

## üê≥ D√©ploiement Docker

### 1. Construire l'image

```bash
# Build avec GPU support
docker build -f Dockerfile.talking_head -t talking-head-api .

# Tag pour votre registry
docker tag talking-head-api votre-username/talking-head-api:latest

# Push
docker push votre-username/talking-head-api:latest
```

### 2. D√©ployer sur RunPod

1. Allez sur [RunPod Serverless](https://www.runpod.io/console/serverless)
2. Cr√©ez un nouveau endpoint
3. Configuration:
   - **Image Docker:** `votre-username/talking-head-api:latest`
   - **GPU:** RTX 3090 ou mieux (pour Wav2Lip/SadTalker)
   - **Idle Timeout:** 30s
   - **Max Workers:** 3-10 selon votre budget

### 3. Variables d'environnement

Configurez dans RunPod:
```bash
ELEVENLABS_API_KEY=votre-cl√©     # Si vous utilisez ElevenLabs
AWS_ACCESS_KEY_ID=votre-cl√©       # Pour upload S3
AWS_SECRET_ACCESS_KEY=votre-secret
S3_BUCKET_NAME=votre-bucket
```

## üí∞ Co√ªts estim√©s

### Par vid√©o (30 secondes):

**Option √©conomique (gTTS + Wav2Lip):**
- TTS: Gratuit
- G√©n√©ration vid√©o: ~$0.02 (GPU RunPod)
- Storage S3: ~$0.001
- **Total: ~$0.02/vid√©o**

**Option premium (ElevenLabs + Wav2Lip):**
- TTS: ~$0.03
- G√©n√©ration vid√©o: ~$0.02
- Storage S3: ~$0.001
- **Total: ~$0.05/vid√©o**

**Option cloud (D-ID):**
- Tout inclus: ~$0.10/vid√©o

## üß™ Tests

### Test local (sans mod√®le)
```bash
python test_talking_head.py
```

### Test avec vraie image
```bash
# Ajoutez une photo
cp votre-photo.jpg test_image.jpg

# Modifiez test_talking_head.py pour d√©commenter les tests complets
python test_talking_head.py
```

## üìä Performance

### Temps de g√©n√©ration typiques:

| Configuration | Temps (30s vid√©o) | Co√ªt |
|--------------|------------------|------|
| gTTS + Wav2Lip (RTX 3090) | ~15s | $0.02 |
| ElevenLabs + Wav2Lip (RTX 3090) | ~20s | $0.05 |
| ElevenLabs + SadTalker (A100) | ~45s | $0.10 |
| D-ID API | ~60s | $0.10 |

## üé® Cas d'usage

- **E-learning:** Avatars pour cours en ligne
- **Marketing:** Vid√©os personnalis√©es √† grande √©chelle
- **Accessibilit√©:** Traduction vid√©o avec lip-sync
- **R√©seaux sociaux:** Contenu automatis√©
- **Service client:** Avatars virtuels 24/7
- **Actualit√©s:** Pr√©sentateurs virtuels

## ‚ö†Ô∏è Limitations et consid√©rations

### Techniques:
- Qualit√© d√©pend de la photo d'entr√©e (visage bien visible)
- Mouvements de t√™te limit√©s avec Wav2Lip
- Cold start: 10-30s la premi√®re fois

### √âthiques:
- ‚ö†Ô∏è **Deepfakes:** Utilisez cette technologie de mani√®re responsable
- Obtenez le consentement avant d'utiliser une photo
- Ajoutez des watermarks pour indiquer le contenu synth√©tique
- Respectez les lois sur l'usurpation d'identit√©

### L√©gales:
- V√©rifiez les droits d'utilisation des photos
- Conformit√© RGPD si donn√©es personnelles
- Certains pays r√©gulent strictement les deepfakes

## üîó Ressources

- [Wav2Lip GitHub](https://github.com/Rudrabha/Wav2Lip)
- [SadTalker GitHub](https://github.com/OpenTalker/SadTalker)
- [D-ID API](https://www.d-id.com/)
- [ElevenLabs](https://elevenlabs.io/)
- [RunPod Documentation](https://docs.runpod.io/)

## üÜò Support

Pour les questions et probl√®mes:
1. V√©rifiez les logs dans le dashboard RunPod
2. Testez localement avec `test_talking_head.py`
3. Consultez la documentation des mod√®les

## üìù TODO / Am√©liorations futures

- [ ] Support multi-visages
- [ ] Gestion du cache des mod√®les
- [ ] Optimisation du cold start
- [ ] Support des √©motions personnalis√©es
- [ ] API de preview (aper√ßu sans g√©n√©ration compl√®te)
- [ ] Batch processing (plusieurs vid√©os en parall√®le)
- [ ] Watermarking automatique
