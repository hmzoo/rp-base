"""
RunPod Serverless - Talking Head API with Coqui TTS
===================================================
G√©n√®re une vid√©o o√π une personne sur une image "lit" un texte.
Utilise Coqui TTS XTTS_v2 pour une qualit√© audio professionnelle.

Input:
    - image: URL ou base64 de l'image de la personne
    - text: Le texte √† faire lire
    - voice: (optionnel) Nom du speaker ou fichier audio pour clonage
    - language: (optionnel) Langue du texte (default: 'fr')

Output:
    - audio_base64: Audio encod√© en base64
    - audio_size_bytes: Taille de l'audio
"""

import runpod
import base64
import os
import tempfile
import requests
from pathlib import Path
import json
import torch
import sys
import subprocess

print(f"üöÄ D√©marrage du worker RunPod")
print(f"üêç Python version: {sys.version}")
print(f"üìç Working directory: {os.getcwd()}")

# V√©rifier les d√©pendances syst√®me
print(f"\nüì¶ V√©rification des d√©pendances syst√®me...")
try:
    result = subprocess.run(['espeak-ng', '--version'], capture_output=True, text=True)
    print(f"‚úÖ espeak-ng install√©")
except Exception as e:
    print(f"‚ö†Ô∏è espeak-ng: {e}")

try:
    import soundfile
    print(f"‚úÖ soundfile (libsndfile1) v{soundfile.__version__}")
except Exception as e:
    print(f"‚ö†Ô∏è soundfile: {e}")

print(f"\nüì¶ Import TTS...")
try:
    from TTS.api import TTS
    print("‚úÖ TTS import√© avec succ√®s")
except Exception as e:
    print(f"‚ùå ERREUR import TTS: {e}")
    import traceback
    traceback.print_exc()
    raise

# Initialisation globale des mod√®les (charg√©s une seule fois)
TTS_MODEL = None
WAV2LIP_MODEL = None

def init_tts_model():
    """Initialise le mod√®le Coqui TTS XTTS_v2"""
    global TTS_MODEL
    
    if TTS_MODEL is None:
        print("\nüîÑ Chargement du mod√®le Coqui TTS XTTS_v2...")
        from TTS.api import TTS
        
        # V√©rifier si GPU disponible
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   üì± Device: {device}")
        if torch.cuda.is_available():
            print(f"   üéÆ GPU: {torch.cuda.get_device_name(0)}")
            print(f"   üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        try:
            # Charger le mod√®le multilingue XTTS_v2
            print(f"   ‚è≥ T√©l√©chargement/chargement du mod√®le (~2GB)...")
            TTS_MODEL = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
            print("   ‚úÖ Mod√®le charg√© avec succ√®s")
        except Exception as e:
            print(f"   ‚ùå ERREUR chargement mod√®le: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    return TTS_MODEL


def download_image(image_input):
    """
    T√©l√©charge ou d√©code l'image d'entr√©e.
    
    Args:
        image_input: URL ou base64 de l'image
    
    Returns:
        str: Chemin vers le fichier image temporaire
    """
    temp_dir = tempfile.mkdtemp()
    image_path = os.path.join(temp_dir, "input_image.jpg")
    
    if image_input.startswith('http://') or image_input.startswith('https://'):
        # T√©l√©charger depuis URL
        response = requests.get(image_input)
        response.raise_for_status()
        with open(image_path, 'wb') as f:
            f.write(response.content)
    elif image_input.startswith('data:image'):
        # D√©coder base64
        header, encoded = image_input.split(',', 1)
        image_data = base64.b64decode(encoded)
        with open(image_path, 'wb') as f:
            f.write(image_data)
    else:
        # Assumer que c'est du base64 sans header
        image_data = base64.b64decode(image_input)
        with open(image_path, 'wb') as f:
            f.write(image_data)
    
    return image_path, temp_dir


def text_to_speech(text, language='fr', voice='Claribel Dervla'):
    """
    Convertit le texte en audio avec Coqui TTS XTTS_v2.
    
    Speakers disponibles par d√©faut:
    - Claribel Dervla (f√©minin, clair)
    - Daisy Studious (f√©minin, pos√©)
    - Gracie Wise (f√©minin, mature)
    - Tammie Ema (f√©minin, jeune)
    - Alison Dietlinde (f√©minin, professionnel)
    - Ana Florence (f√©minin, chaleureux)
    - Annmarie Nele (f√©minin, √©nergique)
    - Asya Anara (f√©minin, doux)
    - Brenda Stern (f√©minin, autoritaire)
    - Gitta Nikolina (f√©minin, amical)
    - Henriette Usha (f√©minin, calme)
    - Sofia Hellen (f√©minin, √©l√©gant)
    - Tammy Grit (f√©minin, dynamique)
    - Tanja Adelina (f√©minin, confiant)
    - Vjollca Johnnie (f√©minin, expressif)
    - Andrew Chipper (masculin, jeune)
    - Badr Odhiambo (masculin, grave)
    - Dionisio Schuyler (masculin, mature)
    - Royston Min (masculin, calme)
    - Viktor Eka (masculin, autoritaire)
    - Abrahan Mack (masculin, chaleureux)
    - Adde Michal (masculin, amical)
    - Baldur Sanjin (masculin, puissant)
    - Craig Gutsy (masculin, √©nergique)
    - Damien Black (masculin, s√©rieux)
    - Gilberto Mathias (masculin, professionnel)
    - Ilkin Urbano (masculin, confiant)
    - Kazuhiko Atallah (masculin, pos√©)
    - Ludvig Milivoj (masculin, doux)
    - Suad Qasim (masculin, expressif)
    
    Clonage de voix:
    - Passez l'URL ou le chemin d'un fichier audio de 3-10 secondes
    
    Args:
        text: Le texte √† synth√©tiser
        language: Code langue (fr, en, es, de, it, pt, pl, tr, ru, nl, cs, ar, zh-cn, ja, hu, ko, hi)
        voice: Nom du speaker ou URL/chemin audio pour clonage
    
    Returns:
        tuple: (audio_path, temp_dir)
    """
    temp_dir = tempfile.mkdtemp()
    audio_path = os.path.join(temp_dir, "speech.wav")
    
    # Initialiser le mod√®le
    tts = init_tts_model()
    
    print(f"   üé§ Synth√®se Coqui TTS: langue={language}, speaker={voice}")
    
    try:
        # V√©rifier si c'est un clonage de voix (URL ou fichier)
        if voice.startswith('http://') or voice.startswith('https://') or os.path.isfile(voice):
            print(f"   üé≠ Clonage de voix depuis: {voice}")
            # T√©l√©charger l'audio de r√©f√©rence si c'est une URL
            if voice.startswith('http'):
                ref_audio = os.path.join(temp_dir, "reference_voice.wav")
                response = requests.get(voice)
                with open(ref_audio, 'wb') as f:
                    f.write(response.content)
                voice = ref_audio
            
            # G√©n√©rer avec clonage
            tts.tts_to_file(
                text=text,
                speaker_wav=voice,
                language=language,
                file_path=audio_path
            )
        else:
            # Utiliser un speaker par d√©faut
            tts.tts_to_file(
                text=text,
                speaker=voice,
                language=language,
                file_path=audio_path
            )
        
        print(f"   ‚úì Audio g√©n√©r√©: {audio_path}")
        return audio_path, temp_dir
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur TTS: {e}")
        # Fallback sur un speaker par d√©faut
        print(f"   üîÑ Tentative avec speaker par d√©faut...")
        tts.tts_to_file(
            text=text,
            speaker="Claribel Dervla",
            language=language,
            file_path=audio_path
        )
        return audio_path, temp_dir


def init_wav2lip_model():
    """Initialise le mod√®le Wav2Lip pour g√©n√©ration vid√©o"""
    global WAV2LIP_MODEL
    
    if WAV2LIP_MODEL is None:
        print("\nüé¨ Chargement du mod√®le Wav2Lip...")
        import sys
        sys.path.append('/app/Wav2Lip')
        
        try:
            from models import Wav2Lip as Wav2LipModel
            import mediapipe as mp
            
            checkpoint_path = '/app/Wav2Lip/checkpoints/wav2lip_gan.pth'
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"   üì± Device: {device}")
            
            # T√©l√©charger le mod√®le s'il n'existe pas
            if not os.path.exists(checkpoint_path):
                print(f"   üì• T√©l√©chargement du mod√®le Wav2Lip (~145 MB)...")
                import urllib.request
                model_url = 'https://github.com/Rudrabha/Wav2Lip/releases/download/models/wav2lip_gan.pth'
                try:
                    urllib.request.urlretrieve(model_url, checkpoint_path)
                    print(f"   ‚úÖ Mod√®le t√©l√©charg√© avec succ√®s")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Tentative URL alternative...")
                    # URL alternative sur Google Drive ou autre CDN
                    alt_url = 'https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?download=1'
                    urllib.request.urlretrieve(alt_url, checkpoint_path)
                    print(f"   ‚úÖ Mod√®le t√©l√©charg√© (URL alternative)")
            
            # Charger le mod√®le
            print(f"   ‚è≥ Chargement du checkpoint Wav2Lip...")
            model = Wav2LipModel()
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # Charger les poids du mod√®le
            s = checkpoint["state_dict"]
            new_s = {}
            for k, v in s.items():
                new_s[k.replace('module.', '')] = v
            model.load_state_dict(new_s)
            
            model = model.to(device)
            model.eval()
            
            # Initialiser MediaPipe Face Detection
            mp_face_detection = mp.solutions.face_detection
            face_detector = mp_face_detection.FaceDetection(min_detection_confidence=0.5)
            
            WAV2LIP_MODEL = {'model': model, 'device': device, 'face_detector': face_detector}
            print("   ‚úÖ Mod√®le Wav2Lip charg√© avec succ√®s")
            
        except Exception as e:
            print(f"   ‚ùå ERREUR chargement Wav2Lip: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    return WAV2LIP_MODEL


def generate_talking_head(image_path, audio_path, output_path):
    """
    G√©n√®re la vid√©o talking head avec Wav2Lip.
    
    Args:
        image_path: Chemin vers l'image
        audio_path: Chemin vers l'audio
        output_path: Chemin de sortie pour la vid√©o
    
    Returns:
        str: Chemin vers la vid√©o g√©n√©r√©e
    """
    import sys
    sys.path.append('/app/Wav2Lip')
    
    import cv2
    import numpy as np
    from os import path
    import audio as wav2lip_audio
    import mediapipe as mp
    
    print("   üé¨ Initialisation Wav2Lip...")
    
    # Charger le mod√®le
    wav2lip_data = init_wav2lip_model()
    model = wav2lip_data['model']
    device = wav2lip_data['device']
    face_detector = wav2lip_data['face_detector']
    
    # Param√®tres
    mel_step_size = 16
    img_size = 96
    fps = 25
    batch_size = 128
    pads = [0, 10, 0, 0]  # top, bottom, left, right
    
    print("   üì∏ D√©tection du visage...")
    
    # Charger l'image
    if not path.isfile(image_path):
        raise ValueError(f'Image non trouv√©e: {image_path}')
    
    # Cr√©er une vid√©o statique √† partir de l'image
    frame = cv2.imread(image_path)
    
    if frame is None:
        raise ValueError(f"Impossible de charger l'image: {image_path}")
    
    # Charger l'audio et calculer les mel spectrograms
    print("   üéµ Traitement de l'audio...")
    wav = wav2lip_audio.load_wav(audio_path, 16000)
    mel = wav2lip_audio.melspectrogram(wav)
    
    # Calculer le nombre de frames n√©cessaires
    mel_chunks = []
    mel_idx_multiplier = 80. / fps
    i = 0
    while True:
        start_idx = int(i * mel_idx_multiplier)
        if start_idx + mel_step_size > len(mel[0]):
            mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])
            break
        mel_chunks.append(mel[:, start_idx: start_idx + mel_step_size])
        i += 1
    
    print(f"   üìä G√©n√©ration de {len(mel_chunks)} frames...")
    
    # Cr√©er les frames de l'image r√©p√©t√©es
    full_frames = [frame.copy() for _ in range(len(mel_chunks))]
    
    # D√©tecter les visages avec MediaPipe
    print("   üë§ D√©tection des visages...")
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = face_detector.process(rgb_frame)
    
    if not results.detections:
        raise ValueError("Aucun visage d√©tect√© dans l'image")
    
    # Utiliser le premier visage d√©tect√©
    detection = results.detections[0]
    bboxC = detection.location_data.relative_bounding_box
    ih, iw, _ = frame.shape
    
    # Convertir les coordonn√©es relatives en pixels
    x1 = int(bboxC.xmin * iw)
    y1 = int(bboxC.ymin * ih)
    w = int(bboxC.width * iw)
    h = int(bboxC.height * ih)
    x2 = x1 + w
    y2 = y1 + h
    
    # Appliquer les paddings
    y1 = max(0, y1 - pads[0])
    y2 = min(ih, y2 + pads[1])
    x1 = max(0, x1 - pads[2])
    x2 = min(iw, x2 + pads[3])
    
    # Extraire la r√©gion du visage
    face_rect = frame[y1:y2, x1:x2]
    
    # Cr√©er face_det_results pour chaque frame (m√™me visage)
    coords = (y1, y2, x1, x2)
    face_det_results = [(face_rect.copy(), coords) for _ in range(len(full_frames))]
    
    print("   üé≠ G√©n√©ration du lip-sync...")
    
    # G√©n√©rer la vid√©o avec lip-sync
    gen = datagen(full_frames.copy(), mel_chunks, face_det_results, img_size, batch_size)
    
    frame_h, frame_w = full_frames[0].shape[:-1]
    out = cv2.VideoWriter(output_path, 
                         cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_w, frame_h))
    
    for i, (img_batch, mel_batch, frames, coords) in enumerate(gen):
        img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)
        mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)
        
        with torch.no_grad():
            pred = model(mel_batch, img_batch)
        
        pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.
        
        for p, f, c in zip(pred, frames, coords):
            y1, y2, x1, x2 = c
            p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))
            f[y1:y2, x1:x2] = p
            out.write(f)
    
    out.release()
    print(f"   ‚úÖ Vid√©o g√©n√©r√©e: {output_path}")
    
    return output_path


def datagen(frames, mels, face_det_results, img_size, batch_size):
    """G√©n√©rateur de batches pour Wav2Lip"""
    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []
    
    for i, m in enumerate(mels):
        idx = i % len(frames)
        frame_to_save = frames[idx].copy()
        face, coords = face_det_results[idx].copy()
        
        face = cv2.resize(face, (img_size, img_size))
        
        img_batch.append(face)
        mel_batch.append(m)
        frame_batch.append(frame_to_save)
        coords_batch.append(coords)
        
        if len(img_batch) >= batch_size:
            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)
            img_batch = (img_batch / 255.) * 2 - 1
            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], 80, -1])
            
            yield img_batch, mel_batch, frame_batch, coords_batch
            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []
    
    if len(img_batch) > 0:
        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)
        img_batch = (img_batch / 255.) * 2 - 1
        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], 80, -1])
        
        yield img_batch, mel_batch, frame_batch, coords_batch


def upload_to_storage(video_path):
    """
    Upload la vid√©o vers un stockage (S3, etc.).
    
    Args:
        video_path: Chemin local de la vid√©o
    
    Returns:
        str: URL publique de la vid√©o
    """
    # TODO: Impl√©menter l'upload vers S3 ou autre
    return f"file://{video_path}"


def handler(event):
    """
    Handler principal pour l'API Talking Head avec Coqui TTS.
    
    Args:
        event: √âv√©nement RunPod contenant:
            - input.image: URL ou base64 de l'image
            - input.text: Texte √† faire lire
            - input.voice: (optionnel) Speaker ou URL audio pour clonage (default: 'Claribel Dervla')
            - input.language: (optionnel) Langue (default: 'fr')
    
    Returns:
        dict: R√©sultat avec audio_base64 et m√©tadonn√©es
    """
    try:
        job_input = event.get('input', {})
        
        # Validation des entr√©es
        if 'image' not in job_input:
            return {'error': 'Le champ "image" est requis (URL ou base64)'}
        
        if 'text' not in job_input:
            return {'error': 'Le champ "text" est requis'}
        
        image_input = job_input['image']
        text = job_input['text']
        language = job_input.get('language', 'fr')
        voice = job_input.get('voice', 'Claribel Dervla')
        
        print(f"üì• Traitement: texte='{text[:50]}...', langue={language}, voix={voice}")
        
        # √âtape 1: T√©l√©charger/d√©coder l'image
        print("1Ô∏è‚É£ T√©l√©chargement de l'image...")
        image_path, image_temp_dir = download_image(image_input)
        print(f"   ‚úì Image sauvegard√©e: {image_path}")
        
        # √âtape 2: G√©n√©rer l'audio (TTS)
        print("2Ô∏è‚É£ G√©n√©ration de l'audio (Coqui TTS XTTS_v2)...")
        audio_path, audio_temp_dir = text_to_speech(text, language, voice)
        
        # Encoder l'audio en base64
        with open(audio_path, 'rb') as audio_file:
            audio_base64 = base64.b64encode(audio_file.read()).decode('utf-8')
            audio_size = os.path.getsize(audio_path)
        
        print(f"   ‚úì Audio encod√©: {audio_size} bytes")
        
        # √âtape 3: G√©n√©rer la vid√©o talking head avec Wav2Lip
        print("3Ô∏è‚É£ G√©n√©ration de la vid√©o talking head (Wav2Lip)...")
        output_dir = tempfile.mkdtemp()
        output_path = os.path.join(output_dir, "output_video.mp4")
        
        try:
            generate_talking_head(image_path, audio_path, output_path)
            print(f"   ‚úì Vid√©o g√©n√©r√©e: {output_path}")
            
            # Encoder la vid√©o en base64
            with open(output_path, 'rb') as video_file:
                video_base64 = base64.b64encode(video_file.read()).decode('utf-8')
                video_size = os.path.getsize(output_path)
            
            print(f"   ‚úì Vid√©o encod√©e: {video_size} bytes")
            
            # Nettoyage
            import shutil
            shutil.rmtree(image_temp_dir, ignore_errors=True)
            shutil.rmtree(audio_temp_dir, ignore_errors=True)
            shutil.rmtree(output_dir, ignore_errors=True)
            
            return {
                'success': True,
                'video_base64': video_base64,
                'video_size_bytes': video_size,
                'audio_base64': audio_base64,
                'audio_size_bytes': audio_size,
                'tts_engine': 'Coqui TTS XTTS_v2',
                'video_engine': 'Wav2Lip GAN',
                'speaker': voice,
                'language': language,
                'text_length': len(text),
                'format': 'mp4'
            }
            
        except Exception as video_error:
            # Si erreur Wav2Lip, retourner juste l'audio
            print(f"   ‚ö†Ô∏è  Erreur g√©n√©ration vid√©o: {video_error}")
            import traceback
            traceback.print_exc()
            
            # Nettoyage
            import shutil
            shutil.rmtree(image_temp_dir, ignore_errors=True)
            shutil.rmtree(audio_temp_dir, ignore_errors=True)
            if os.path.exists(output_dir):
                shutil.rmtree(output_dir, ignore_errors=True)
            
            return {
                'success': False,
                'error': 'Vid√©o non g√©n√©r√©e (voir logs)',
                'error_details': str(video_error),
                'audio_generated': True,
                'audio_base64': audio_base64,
                'audio_size_bytes': audio_size,
                'tts_engine': 'Coqui TTS XTTS_v2',
                'speaker': voice,
                'language': language
            }
        
    except Exception as e:
        import traceback
        print(f"‚ùå ERREUR: {e}")
        traceback.print_exc()
        return {
            'error': str(e),
            'type': type(e).__name__,
            'traceback': traceback.format_exc()
        }


if __name__ == "__main__":
    # Mode d√©veloppement: test local
    print("üöÄ D√©marrage du worker RunPod - Talking Head API (Coqui TTS)")
    print("=" * 60)
    
    # D√©marrer le worker
    runpod.serverless.start({"handler": handler})
